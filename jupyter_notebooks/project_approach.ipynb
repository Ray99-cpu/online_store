{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Project Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "#### To explain the approach taken and the challenges and the solutions found.This will focus on the following areas:\n",
        "\n",
        "\n",
        "* Data management\n",
        "\n",
        "* Rationale behind the code\n",
        "\n",
        "* Methods chosen, limitations,and alternatives\n",
        "\n",
        "* Data limitations and challenges\n",
        "\n",
        "* Data handling\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The dataset was chosen from the Kaggle website after reading its description and its possible uses, and then downloaded. The dataset was stored and then loaded from the raw data folder into an appropriately named Jupyter notebook, named dataset resized, and resized and given an appropriate file name of ecommerce_transactions_resized and stored in the cleaned_data folder. The dataset was loaded into another Jupyter notebook, named dataset_clean_modify, from the cleaned_data folder, and modified in a Jupyter notebook using the appropriate code. The dataset was then given an appropriate file name, commerce_transactions_cleaned, and stored in the cleaned_data folder. The dataset has been balanced for a fraud detection activity, and AI tools have been used to aid this task. The file was named and stored in the balanced data folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "### Rationale behind the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The rationale behind the code above was improved by using variable names that were appropriate to the task at hand. I improved the performance by only using code that focused on variables that I was interested in, like using  the following: df.groupby(‘Country’)[‘Purchase_Amount’].sum().reset_index(name=’Purchase by Country’). This was more efficient in obtaining the data and the sight that I required. The errors that were outputted related to the incorrect naming of the DataFrame or the variable, where an uppercase letter was required. This was brought to my attention when I read the error message, which stated that there was an attribute error or that a library had not been installed. On correcting this, the code ran smoothly. I did, however, read through the code just to check that the syntax and spelling were correct, but there were times when it was missed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Methods chosen, limitations,and alternatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### I chose the method of selecting certain variables because it would extract the data I required for a given business requirement and insight. This approach would cycle through the DataFrame and extract the required output. As an alternative approach, I could have just taken a sample of the data, which would have been more efficient to process, but not truly reflective of the DataFrame's content. When hypothesis testing using Chi and Welch’s t-test, the outcomes were clear, but the result was not significant due to the contents of the dataset. I had to change the date format to a more granular format to derive better insights. I would also input my code into Copilot or ChatGPT just to see if I was correct. There were times when I was and other times when the code required a little tweaking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data limitations and challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The challenges faced during the data analysis were the dataset itself, because it was not very comprehensive. I would have liked to have data on GDP for each country and economic information regarding inflation, interest rates. These factors have an impact on the amount of disposable income that a nation's population is likely to have. To widen the scope of the analysis, I could have used data from the LSEG Data & Analytics website to have a more extensive dataset, but there is a cost for this."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Nov  7 2022, 16:45:55) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
